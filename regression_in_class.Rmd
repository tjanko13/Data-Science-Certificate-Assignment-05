---
title: "Regression in Data Analysis"
author: "Scott Stoltzman"
date: "5/22/2019"
output: html_document
---

```{r setup, include=FALSE}
library('janitor')
library('lubridate')
library('tidyverse')
```

### Situation:  

A bike sharing company has provided you with data from <https://data.world/data-society/capital-bikeshare-2011-2012>. They would like you to analyze it and help them forecast the number of people they can expect to see, hourly, each day on their bikes. They will have a forecast of weather variables that they consider to be accurate and are willing to accept the risk involved with changes in weather.

### Read in data
```{r}
raw_dat = read_csv('https://foco-ds-portal-files.s3.amazonaws.com/bike+data.csv')
```

### Look at data
```{r}
head(raw_dat)
```

### What could be better?
```{r}
# Names are capitalized
raw_dat %>%
  clean_names()
```

```{r}
# Dates are not actually dates
raw_dat %>%
  mutate(Date = mdy(Date))
```


```{r}
dat = raw_dat %>%
  clean_names() %>%
  mutate(date = mdy(date))
dat
```


#### Now that your data is easier to handle, perform some EDA
```{r}
dat %>%
  summary()
```

```{r}
colSums(is.na(dat))
```


Plot the `total_users` by `date`
```{r}
dat %>% 
  ggplot() + 
  geom_line(aes(x = date, y = total_users))
```


Huh??? why did that happen?
```{r}
dat %>%
  group_by(date) %>%
  summarize(total_users = sum(total_users)) %>%
  ggplot(aes(x = date, y = total_users)) + 
  geom_line() + 
  geom_smooth(method = 'loess') +
  ggtitle('Total Daily Users 2011 - 2012')
```


What does the distribution of daily users per hour look like?
```{r}
dat %>%
  group_by(date, hour) %>%
  summarize(avg_users = mean(total_users)) %>%
  ggplot(aes(x = avg_users)) + 
  geom_histogram(bins = 30) + 
  ggtitle('Average Daily Users Per Hour')
```


What's the distribution of average users by day of the week look like?
```{r}
dat %>%
  group_by(date, hour, day_of_the_week) %>%
  summarize(avg_users = mean(total_users)) %>%
  ggplot(aes(x = factor(day_of_the_week), y = avg_users)) + 
  geom_boxplot()
```


What does a working day vs. non-working day look like per hour in terms of average users?
```{r}
dat %>%
  group_by(hour, working_day) %>%
  summarize(avg_users = mean(total_users)) %>%
  ggplot(aes(x = hour, y = avg_users, col = factor(working_day))) + 
  geom_line() + 
  ggtitle('Average Users Per Hour') + 
  labs(subtitle = 'Broken down by working day')
```


Since we will be using temperatures, let's look at the relationship between `temperature_f` and `temperature_feels_f`
```{r}
dat %>%
  ggplot(aes(x = temperature_f, y = temperature_feels_f)) + 
  geom_point()
```


Let's try to figure out what the deal is with the outliers...
```{r}
dat %>% 
  filter(temperature_f > 60 & temperature_f < 90) %>%
  filter(temperature_feels_f > 25 & temperature_feels_f < 40)
```


The error seems clear, but let's take a look at some of the data before jumping to a conclusion
```{r}
dat %>%
  mutate(tmp_diff = abs(temperature_f - temperature_feels_f)) %>%
  ggplot(aes(x = tmp_diff)) + 
  geom_histogram(binwidth = 1)
```


We do not want to throw out the data. We are probably safe to make some assumptions about imputing the values. However, that requires a bit of manipulation to make `NA` where the temperature doesn't seem accurate. Let's add the `NA`
```{r}
dat %>%
  mutate(tmp_diff = abs(temperature_f - temperature_feels_f),
         tmp_feel_normal = if_else(tmp_diff < 25, temperature_feels_f, NA_real_)) %>%
  filter(is.na(tmp_feel_normal)) %>% 
  select(temperature_f, temperature_feels_f, tmp_diff, tmp_feel_normal)
```


We need to replace our `NA` values with the mean of the other data, grouped by all of the grouping variables. Let's call our new dataset `clean_dat`.
```{r}
clean_dat = dat %>% 
  mutate(tmp_diff = abs(temperature_f - temperature_feels_f),
         tmp_feel_normal = if_else(tmp_diff < 25, temperature_feels_f, NA_real_)) %>%
  group_by(season, hour, holiday, day_of_the_week, working_day, weather_type) %>%
  mutate(temperature_feels_f = if_else(is.na(tmp_feel_normal), mean(temperature_feels_f, na.rm = TRUE), temperature_feels_f)) %>%
  ungroup()
```


Create the same plot in which we discovered the inconsistencies in the data. Notice what happened to the outliers.
```{r}
clean_dat %>%
  ggplot(aes(x = temperature_f, y = temperature_feels_f)) + 
  geom_point()
```




Let's plot our temperature data again, this time we can group by `date` and use the average of our data. Would you say these two variables are correlated?
```{r}
clean_dat %>%
  group_by(date) %>%
  summarize(mean_tmp = mean(temperature_f),
            mean_tmp_feels = mean(temperature_feels_f),
            avg_users = mean(total_users)) %>%
  ggplot(aes(x = mean_tmp, y = mean_tmp_feels, 
             size = avg_users, col = avg_users, fill = avg_users)) + 
  geom_point(alpha = 0.3)
```


What is the relationship between average temperature and average users?
```{r}
clean_dat %>% 
  group_by(date) %>%
  summarize(avg_tmp_feels = mean(temperature_feels_f),
            avg_tmp = mean(temperature_f),
            avg_users = mean(total_users)) %>%
  ggplot(aes(x = avg_tmp, y = avg_users)) + 
  geom_point()
```


What is the relationship between average temperature feels like and average users?
```{r}
clean_dat %>% 
  group_by(date) %>%
  summarize(avg_tmp_feels = mean(temperature_feels_f),
            avg_tmp = mean(temperature_f),
            avg_users = mean(total_users)) %>%
  ggplot(aes(x = avg_tmp_feels, y = avg_users)) + 
  geom_point()
```


Plotting all of the variables against each other can sometimes help to find correlations.
```{r}
temporary = clean_dat %>%
  group_by(date) %>%
  summarize(temperature_feels_f = mean(temperature_feels_f), 
            humidity = mean(humidity), 
            wind_speed = mean(wind_speed), 
            total_users = mean(total_users))
plot(temporary)
```




We will call temperature feels like `x` and total users `y` to make our introduction to linear regression easier.
```{r}
reg_dat = tibble(x = clean_dat$temperature_feels_f, y = clean_dat$total_users)
reg_dat
```


### y = mx + b
Now we create the formula for our line of best fit. We will just guess some slope and intercept numbers to start off with.
```{r}
slope = 10
intercept = -9

model = reg_dat %>%
  mutate(fit = (slope * x) + intercept)

head(model)
```


```{r}
model %>%
  ggplot(aes(x = x)) + 
  geom_point(aes(y = y), alpha = 0.10) + 
  geom_line(aes(y = fit), col = 'blue', size = 2)
```


Let's modify those numbers and see how our chart looks.
```{r}
slope = 4
intercept = -25

model = reg_dat %>%
  mutate(fit = (slope * x) + intercept)

model %>%
  ggplot(aes(x = x)) + 
  geom_point(aes(y = y), alpha = 0.10) + 
  geom_line(aes(y = fit), col = 'blue', size = 2)
```


Residuals are important. They tell us how far off the fit is from the true point.
```{r}
model %>% 
  mutate(residuals = fit - y)
```


Let's take a look at what a histogram of the residuals looks like. A good result looks like it's centered around 0 and is "normally" distributed.
```{r}
slope = 4
intercept = -25

model = reg_dat %>%
  mutate(fit = (slope * x) + intercept)

model %>% 
  mutate(residuals = fit - y) %>%
  ggplot(aes(x = residuals)) + 
  geom_histogram(bins = 30)
```


Let's do this for a whole bunch of models to prove a point.
```{r}
slope = 1
intercept = -10
model1 = reg_dat %>%
  mutate(fit = (slope * x) + intercept,
         model = 'model1')


slope = 4
intercept = -20
model2 = reg_dat %>%
  mutate(fit = (slope * x) + intercept,
         model = 'model2')


slope = 10
intercept = -25
model3 = reg_dat %>%
  mutate(fit = (slope * x) + intercept,
         model = 'model3')


all_models = bind_rows(model1, model2, model3) 

all_models %>% 
  mutate(residuals = fit - y) %>%
  ggplot(aes(x = residuals, fill = model)) + 
  geom_density(alpha = 0.3)
```


We need to know the total magnitude of the error, what's wrong with doing this?
```{r}
all_models %>%
  mutate(residuals = fit - y) %>%
  group_by(model) %>%
  summarize(sum_of_error = sum(residuals))
```


We use the sum of squared error in order to account for both positive and negative errors! That way everything is positive.
```{r}
all_models %>%
  mutate(residuals = fit - y,
         residuals_squared = residuals^2) %>%
  group_by(model) %>%
  summarize(sum_of_squared_error = sum(residuals_squared))
```



We will run a bunch of different intercepts and slopes and find the sum of squared errors.
```{r}
slope = -1
intercept = -10
model = reg_dat %>%
  mutate(fit = (slope * x) + intercept,
         residuals = fit - y,
         residuals_squared = residuals^2,
         slope = slope,
         intercept = intercept,
         slope_intercept = paste0(as.character(slope), '_', as.character(intercept)))


slope = 1:4
intercept = -1:1
for(s in slope){
  for(i in intercept){
    temporary = reg_dat %>%
      mutate(fit = (s * x) + i,
             residuals = fit - y,
             residuals_squared = residuals^2,
             slope = s,
             intercept = i,
             slope_intercept = paste0(as.character(slope), '_', as.character(intercept)))
    model = bind_rows(model, temporary)
  }
}

model
```

```{r}
model %>%
  ggplot(aes(x = x, group = slope_intercept)) + 
  geom_point(aes(y = y)) + 
  geom_line(aes(y = fit), alpha = 0.3, col = 'red')
```


### R can do this for us

```{r}
mod = lm(total_users ~ temperature_feels_f, data = clean_dat)
summary(mod)
```

The slope tells us how much 1 unit change in `temperature_feels_f` relates to change in `total_users`. So in this case, 3.5 more users for every 1 degree increase! They are huge global warming fans...

R-squared will tell us how good the line fits. The closer to 1 it is, the better. We will talk about the other measurements as we move forward in comparing models!
